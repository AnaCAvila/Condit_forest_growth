colnames(df) <- c('a', 'b', 'a_hat', 'b_hat')
df <- as.data.frame(df)
plot(df$a, df$a_hat)
abline(0, 1, col = 'red')
plot(df$b, df$b_hat)
abline(0, 1, col = 'red')
#Now, plot the distributions of the real values around the expected:
pars <- c(runif(1, 1, 10), runif(1, 1, 10))
data = data.frame(x = runif(50, 0, 30))
data$R <- pars[1] * data$x + pars[2] * data$y  + rnorm(50,0,3)
#plot(data$x, data$R)
#curve(model_iter(pars, x), 0, 30, add = TRUE)
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
#diff <- model_iter(o$par, data$x) - data$y
#plot(density(diff))
# shapiro.test(diff)
#
# qqnorm(diff)
# qqline(diff, col = 2)
#looks like the errors are indeed normally distributed, so it looks fine.
#writing it with a linear model to understand the logic.
# Faking condit data
# Passing through MLE function
# Dec 2021
data = data.frame(x = runif(50, 0, 30))
#functions
model_iter <- function(pars, x) {
pars[1] + pars[2] * x
}
NLL <- function(pars, data) {
# Values prediced by the model
if(tail(pars, 1) < 0){ #avoiding NAs by keeping the st dev positive
return(-Inf)
}
pred <- model_iter(pars, data$x)
# Negative log-likelihood
fun <- -sum(dnorm(x = data$y, mean = pred, sd = tail(pars, 1), log = TRUE))
#print(pars)
return(fun)
}
mle_optim_LM <- function(data) {
pars <- c(runif(1, 1, 10), runif(1, 1, 10))
data$y <- pars[1] + pars[2] * data$x + rnorm(50,0,3)
#plot(data$x, data$y)
#curve(model_iter(pars, x), 0, 30, add = TRUE)
par0 <- c(5,5,1)
NLL(par0, data=data)
#curve(model_iter(par0, x), 0, 30, add = TRUE, col = 'green')
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
o
#plot(data$x, data$y)
#curve(model_iter(o$par, x), 0, 30, col = "blue", add = TRUE)
iter <- c(pars[1], pars[2], o$par[1], o$par[2])
return(iter)
}
df <- replicate(n = 100, mle_optim_LM(data = data))
df <- t(df)
colnames(df) <- c('a', 'b', 'a_hat', 'b_hat')
df <- as.data.frame(df)
plot(df$a, df$a_hat)
abline(0, 1, col = 'red')
plot(df$b, df$b_hat)
abline(0, 1, col = 'red')
#Now, plot the distributions of the real values around the expected:
pars <- c(runif(1, 1, 10), runif(1, 1, 10))
data = data.frame(x = runif(50, 0, 30))
data$y <- pars[1] + pars[2] * data$x  + rnorm(50,0,3)
plot(data$x, data$y)
curve(model_iter(pars, x), 0, 30, add = TRUE)
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
diff <- model_iter(o$par, data$x) - data$y
plot(density(diff))
shapiro.test(diff)
qqnorm(diff)
qqline(diff, col = 2)
#looks like the errors are indeed normally distributed, so it looks fine.
head(data)
pars <- c(runif(1, 1, 10), runif(1, 1, 10))
data$y <- pars[1] + pars[2] * data$x + rnorm(50,0,3)
plot(data$x, data$y)
pars
par0 <- c(5,5,1)
NLL(par0, data=data)
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
o
pars <- c(runif(1, 1, 10), runif(1, 1, 10))
data$y <- pars[1] + pars[2] * data$x + rnorm(50,0,3)
plot(data$x, data$y)
#curve(model_iter(pars, x), 0, 30, add = TRUE)
par0 <- c(5,5,1)
NLL(par0, data=data)
#curve(model_iter(par0, x), 0, 30, add = TRUE, col = 'green')
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
o
mle_optim_LM <- function(data) {
pars <- c(runif(1, 10, 20), runif(1, 1, 10))
data$y <- pars[1] + pars[2] * data$x + rnorm(50,0,3)
#plot(data$x, data$y)
#curve(model_iter(pars, x), 0, 30, add = TRUE)
par0 <- c(5,5,1)
NLL(par0, data=data)
#curve(model_iter(par0, x), 0, 30, add = TRUE, col = 'green')
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
o
#plot(data$x, data$y)
#curve(model_iter(o$par, x), 0, 30, col = "blue", add = TRUE)
iter <- c(pars[1], pars[2], o$par[1], o$par[2])
return(iter)
}
df <- replicate(n = 100, mle_optim_LM(data = data))
df <- t(df)
colnames(df) <- c('a', 'b', 'a_hat', 'b_hat')
df <- as.data.frame(df)
plot(df$a, df$a_hat)
abline(0, 1, col = 'red')
plot(df$b, df$b_hat)
abline(0, 1, col = 'red')
model_iter <- function(pars, x) {
pars[1] + pars[2] * x
}
NLL <- function(pars, data) {
# Values prediced by the model
if(tail(pars, 1) < 0){ #avoiding NAs by keeping the st dev positive
return(-Inf)
}
pred <- model_iter(pars, data$x)
# Negative log-likelihood
fun <- -sum(dnorm(x = data$y, mean = pred, sd = tail(pars, 1), log = TRUE))
#print(pars)
return(fun)
}
mle_optim_LM <- function(data) {
pars <- c(runif(1, 10, 20), runif(1, 1, 10))
data$y <- pars[1] + pars[2] * data$x + rnorm(50,0,3)
#plot(data$x, data$y)
#curve(model_iter(pars, x), 0, 30, add = TRUE)
par0 <- c(5,5,1)
NLL(par0, data=data)
#curve(model_iter(par0, x), 0, 30, add = TRUE, col = 'green')
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
o
#plot(data$x, data$y)
#curve(model_iter(o$par, x), 0, 30, col = "blue", add = TRUE)
iter <- c(pars[1], pars[2], o$par[1], o$par[2])
return(iter)
}
df <- replicate(n = 100, mle_optim_LM(data = data))
df <- t(df)
colnames(df) <- c('a', 'b', 'a_hat', 'b_hat')
df <- as.data.frame(df)
plot(df$a, df$a_hat)
abline(0, 1, col = 'red')
plot(df$b, df$b_hat)
abline(0, 1, col = 'red')
model_iter <- function(pars, x) {
pars[1] + pars[2] * x
}
NLL <- function(pars, data) {
# Values prediced by the model
if(tail(pars, 1) < 0){ #avoiding NAs by keeping the st dev positive
return(-Inf)
}
pred <- model_iter(pars, data$x)
# Negative log-likelihood
fun <- -sum(dnorm(x = data$y, mean = pred, sd = tail(pars, 1), log = TRUE))
#print(pars)
return(fun)
}
mle_optim_LM <- function(data) {
pars <- c(runif(1, 10, 20), runif(1, 1, 10))
data$y <- pars[1] + pars[2] * data$x + rnorm(50,0,3)
#plot(data$x, data$y)
#curve(model_iter(pars, x), 0, 30, add = TRUE)
par0 <- c(5,5,1)
NLL(par0, data=data)
#curve(model_iter(par0, x), 0, 30, add = TRUE, col = 'green')
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
o
#plot(data$x, data$y)
#curve(model_iter(o$par, x), 0, 30, col = "blue", add = TRUE)
iter <- c(pars[1], pars[2], o$par[1], o$par[2])
return(iter)
}
df <- replicate(n = 100, mle_optim_LM(data = data))
df <- t(df)
colnames(df) <- c('a', 'b', 'a_hat', 'b_hat')
df <- as.data.frame(df)
plot(df$a, df$a_hat)
abline(0, 1, col = 'red')
plot(df$b, df$b_hat)
abline(0, 1, col = 'red')
model_iter <- function(pars, x) {
pars[1] + pars[2] * x
}
NLL <- function(pars, data) {
# Values prediced by the model
if(tail(pars, 1) < 0){ #avoiding NAs by keeping the st dev positive
return(-Inf)
}
pred <- model_iter(pars, data$x)
# Negative log-likelihood
fun <- -sum(dnorm(x = data$y, mean = pred, sd = tail(pars, 1), log = TRUE))
#print(pars)
return(fun)
}
mle_optim_LM <- function(data) {
pars <- c(runif(1, 10, 20), runif(1, 1, 10))
data$y <- pars[1] + pars[2] * data$x + rnorm(50,0,3)
#plot(data$x, data$y)
#curve(model_iter(pars, x), 0, 30, add = TRUE)
par0 <- c(5,5,1)
NLL(par0, data=data)
#curve(model_iter(par0, x), 0, 30, add = TRUE, col = 'green')
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
o
#plot(data$x, data$y)
#curve(model_iter(o$par, x), 0, 30, col = "blue", add = TRUE)
iter <- c(pars[1], pars[2], o$par[1], o$par[2])
return(iter)
}
df <- replicate(n = 100, mle_optim_LM(data = data))
df <- t(df)
colnames(df) <- c('a', 'b', 'a_hat', 'b_hat')
df <- as.data.frame(df)
plot(df$a, df$a_hat)
abline(0, 1, col = 'red')
plot(df$b, df$b_hat)
abline(0, 1, col = 'red')
#writing it with a linear model to understand the logic.
# Faking condit data
# Passing through MLE function
data = data.frame(x = runif(50, 0, 30))
#functions
model_iter <- function(pars, x) {
pars[1] + pars[2] * x
}
NLL <- function(pars, data) {
# Values prediced by the model
if(tail(pars, 1) < 0){ #avoiding NAs by keeping the st dev positive
return(-Inf)
}
pred <- model_iter(pars, data$x)
# Negative log-likelihood
fun <- -sum(dnorm(x = data$y, mean = pred, sd = tail(pars, 1), log = TRUE))
#print(pars)
return(fun)
}
mle_optim_LM <- function(data) {
pars <- c(runif(1, 10, 20), runif(1, 1, 10))
data$y <- pars[1] + pars[2] * data$x + rnorm(50,0,3)
#plot(data$x, data$y)
#curve(model_iter(pars, x), 0, 30, add = TRUE)
par0 <- c(5,5,1)
NLL(par0, data=data)
#curve(model_iter(par0, x), 0, 30, add = TRUE, col = 'green')
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
o
#plot(data$x, data$y)
#curve(model_iter(o$par, x), 0, 30, col = "blue", add = TRUE)
iter <- c(pars[1], pars[2], o$par[1], o$par[2])
return(iter)
}
df <- replicate(n = 100, mle_optim_LM(data = data))
df <- t(df)
colnames(df) <- c('a', 'b', 'a_hat', 'b_hat')
df <- as.data.frame(df)
plot(df$a, df$a_hat)
abline(0, 1, col = 'red')
plot(df$b, df$b_hat)
abline(0, 1, col = 'red')
#Now, plot the distributions of the real values around the expected:
pars <- c(runif(1, 1, 10), runif(1, 1, 10))
data = data.frame(x = runif(50, 0, 30))
data$y <- pars[1] + pars[2] * data$x  + rnorm(50,0,3)
plot(data$x, data$y)
curve(model_iter(pars, x), 0, 30, add = TRUE)
o <- optim(par = par0, fn = NLL, data = data, control = list(parscale = abs(par0)),
hessian = FALSE, method = "BFGS")
diff <- model_iter(o$par, data$x) - data$y
plot(density(diff))
shapiro.test(diff)
qqnorm(diff)
qqline(diff, col = 2)
#looks like the errors are indeed normally distributed, so it looks fine.
condit_yearly[which(condit_yearly$plot == ages$Plot.Name)]
data <- subset[ages, age < 3]
data <- subset[ages, ages$age < 3]
data <- subset(ages, ages$age < 3)
data <- subset(ages, age < 3)
data <- subset(ages, Age < 3)
ages <- subset(ages, Age < 3)
data <- subset(condit_yearly, plot %in% ages$Age)
head(condit_yearly)
unique(condit_yearly$plot)
"p07" %in% condit_yearly$plot
View(ages)
data <- subset(condit_yearly, plot %in% ages$Plot.Name)
head(data)
head(data)
ggplot(data, aes(x=year, y=sum_dbh, color = plot, group=plot)) +
xlab('Date') + ylab('Sum DBH') +
ggtitle('Sum DBH vs Time')+
ylim(0, 200000) +
geom_line()+
geom_point(size=1)
data <- rbind(subset(condit_yearly, plot == 'cocoli'))
data <- subset(condit_yearly, plot %in% ages$Plot.Name)
data <- rbind(data, subset(condit_yearly, plot == 'cocoli'))
data <- rbind(data, subset(condit_yearly, plot == 'cocoli'), subset(condit_yearly, plot == 'sherman'))
head(data)
ggplot(data, aes(x=year, y=sum_dbh, color = plot, group=plot)) +
xlab('Date') + ylab('Sum DBH') +
ggtitle('Sum DBH vs Time')+
ylim(0, 200000) +
geom_line()+
geom_point(size=1)
ggplot(data, aes(x=year, y=sum_dbh, color = plot, group=plot)) +
xlab('Date') + ylab('Sum DBH') +
ggtitle('Sum DBH vs Time')+
geom_line()+
geom_point(size=1)
ggplot(data, aes(x=year, y=sum_dbh, color = plot, group=plot)) +
xlab('Date') + ylab('Sum DBH') +
ggtitle('Sum DBH vs Time')+
geom_line()+
geom_point(size=1) +
geom_text(aes(label=ifelse(plot == "cocoli",as.character(plot),'')),hjust=0,vjust=0)
data <- subset(condit_yearly, plot %in% ages$Plot.Name)
data <- rbind(data, subset(condit_yearly, plot == 'cocoli' | plot == 'sherman'))
head(data)
data <- subset(condit_yearly, plot %in% ages$Plot.Name)
data <- rbind(data, subset(condit_yearly, plot == 'cocoli'), subset(condit_yearly, plot == 'sherman'))
data <- subset(condit_yearly, plot %in% ages$Plot.Name)
data <- rbind(data, subset(condit_yearly, plot == 'cocoli' | plot == 'sherman'))
data <- subset(condit_yearly, plot %in% ages$Plot.Name)
data <- rbind(data, subset(condit_yearly, plot == 'cocoli' | plot == 'sherman'))
data <- subset(condit_yearly, plot %in% ages$Plot.Name)
data <- rbind(data, subset(condit_yearly, plot == 'cocoli'), subset(condit_yearly, plot == 'sherman'))
data <- rbind(subset(condit_yearly, plot %in% ages$Plot.Name), subset(condit_yearly, plot == 'cocoli' | plot == 'sherman'))
#get annual means of each variable per plot
condit_yearly <- aggregate(cbind(dbh,agb,ba)  ~ year + plot, condit, mean)
condit_yearly <- rename(condit_yearly, c(mean_dbh = dbh,
mean_agb = agb,
mean_ba = ba))
condit_yearly <- merge(condit_yearly, condit)
head(condit_yearly)
condit_yearly <- subset(condit_yearly, select=-c(dbh,agb,ba,hom,stemID,date,gx,gy,sp)) #remove columns that considered individual stem-level changes at sub-year resolution
condit_yearly <- condit_yearly[order(condit_yearly$plot),]
condit_yearly <- unique(condit_yearly)
head(condit_yearly)
ages$Plot.Name <- tolower(ages$Plot.Name)
ages <- subset(ages, Age < 3)
#include only the plots that are shown to be secondary.
data <- rbind(subset(condit_yearly, plot %in% ages$Plot.Name),
subset(condit_yearly, plot == 'cocoli' | plot == 'sherman'))
ggplot(data, aes(x=year, y=sum_dbh, color = plot, group=plot)) +
xlab('Date') + ylab('Sum DBH') +
ggtitle('Sum DBH vs Time')+
geom_line()+
geom_point(size=1) +
geom_text(aes(label=ifelse(plot == "cocoli",as.character(plot),'')),hjust=0,vjust=0)
ggplot(data, aes(x=year, y=mean_dbh, color = plot, group=plot)) +
xlab('Date') + ylab('Mean DBH') +
ggtitle('Mean DBH vs Time')+
geom_line()+
geom_point(size=1) +
geom_text(aes(label=ifelse(plot == "cocoli",as.character(plot),'')),hjust=0,vjust=0)
head(condit)
data <- data.frame(
person_id = c(10, 11, 15, 12, 10, 13, 10, 11, 12, 14, 15),
study_id = 1:11
)
table(rowSums(with(data, table(person_id, study_id))>0))
# 1 2 3
# 2 3 1
data
head(condit)
data <- condit %>%
group_by(plot) %>%
group_by(year)%>%
summarise(stemID = n())
data
head(data)
data <- condit %>%
group_by(plot) %>%
group_by(year, add=TRUE)%>%
summarise(stemID = n())
head(data)
data <- condit %>%
group_by(plot) %>%
group_by(year, .add=TRUE)%>%
summarise(stemID = n())
data
condit_yearly_num_trees <- condit %>%
group_by(plot) %>%
group_by(year, .add=TRUE)%>%
summarise(stemID = n())
condit_yearly_num_trees <- as.data.frame(condit_yearly_num_trees)
head(condit_yearly_num_trees)
class(condit_yearly_num_trees)
ggplot(condit_yearly_num_trees, aes(x=year, y=stemID, color = plot, group=plot)) +
xlab('Date') + ylab('Total # of trees') +
ggtitle('Number of trees vs Time')+
geom_line()+
geom_point(size=1) +
geom_text(aes(label=ifelse(plot == "cocoli",as.character(plot),'')),hjust=0,vjust=0)
data <- rbind(subset(condit, plot %in% ages$Plot.Name),
subset(condit, plot == 'cocoli' | plot == 'sherman'))
condit <- rbind(subset(condit, plot %in% ages$Plot.Name),
subset(condit, plot == 'cocoli' | plot == 'sherman'))
#look at what's happening at plots that have odd changes of max DBH.
by(condit, condit$plot, function(x){return(tapply(x$dbh,x$year,max))})
condit_yearly <- aggregate(cbind(dbh,agb,ba)  ~ year + plot, condit, sum)
condit_yearly <- rename(condit_yearly, c(sum_dbh = dbh,
sum_agb = agb,
sum_ba = ba))
condit_yearly <- merge(condit_yearly, condit)
head(condit_yearly)
condit_yearly <- subset(condit_yearly, select=-c(dbh,agb,ba,hom,stemID,date,gx,gy,sp)) #remove columns that considered individual stem-level changes at sub-year resolution
condit_yearly <- condit_yearly[order(condit_yearly$plot),]
condit_yearly <- unique(condit_yearly)
#plot sherman, cocoli and p18 (distribution over time)
sherman <- subset(condit, plot == 'sherman')
cocoli <- subset(condit, plot == 'cocoli')
p18 <- subset(condit, plot == 'p18')
head(sherman)
ggplot(sherman, aes(x=stemID)) +
geom_histogram(aes(y=..density..), colour="black", fill="white")+
geom_density(alpha=.2, fill="#FF6666")
ggplot(sherman, aes(x=dbh)) +
geom_histogram(aes(y=..density..), colour="black", fill="white")+
geom_density(alpha=.2, fill="#FF6666")
ggplot(sherman, aes(x=dbh)) +
geom_histogram(aes(y=..density..), colour="black", fill="white", binwidth=15)+
geom_density(alpha=.2, fill="#FF6666")
ggplot(sherman, aes(x=dbh)) +
geom_histogram(aes(y=..density..), colour="black", fill="white", binwidth=15, xlim = c(0,500))+
geom_density(alpha=.2, fill="#FF6666")
ggplot(sherman, aes(x=dbh)) +
geom_histogram(aes(y=..density..), colour="black", fill="white", binwidth=15)+
xlim(0, 500) +
geom_density(alpha=.2, fill="#FF6666")
ggplot(cocoli, aes(x=dbh)) +
geom_histogram(aes(y=..density..), colour="black", fill="white", binwidth=15)+
xlim(0, 500) +
geom_density(alpha=.2, fill="#FF6666")
ggplot(p18, aes(x=dbh)) +
geom_histogram(aes(y=..density..), colour="black", fill="white", binwidth=15)+
xlim(0, 500) +
geom_density(alpha=.2, fill="#FF6666")
data <- rbind(subset(condit, condit$dbh >= 100),
subset(condit, plot != 'cocoli' | plot != 'sherman'))
head(data)
ggplot(data, aes(x=year, y=dbh, color = plot, group=plot)) +
xlab('Date') + ylab('Total # of trees') +
ggtitle('Number of trees vs Time')+
geom_line()+
geom_point(size=1)
head(data)
condit_yearly <- aggregate(cbind(dbh,agb,ba)  ~ year + plot, data, sum)
condit_yearly <- rename(condit_yearly, c(sum_dbh = dbh,
sum_agb = agb,
sum_ba = ba))
condit_yearly <- merge(condit_yearly, data)
head(condit_yearly)
condit_yearly <- subset(condit_yearly, select=-c(dbh,agb,ba,hom,stemID,date,gx,gy,sp)) #remove columns that considered individual stem-level changes at sub-year resolution
condit_yearly <- condit_yearly[order(condit_yearly$plot),]
condit_yearly <- unique(condit_yearly)
ggplot(condit_yearly, aes(x=year, y=mean_dbh, color = plot, group=plot)) +
xlab('Date') + ylab('Mean DBH') +
ggtitle('Mean DBH vs Time')+
geom_line()+
geom_point(size=1)
head(condit_yearly)
ggplot(condit_yearly, aes(x=year, y=sum_dbh, color = plot, group=plot)) +
xlab('Date') + ylab('sum DBH') +
ggtitle('sum DBH vs Time')+
geom_line()+
geom_point(size=1)
head(data)
unique(data$plot)
data <- rbind(subset(condit, condit$dbh >= 100),
subset(condit, plot != 'cocoli'),
subset(condit, plot != 'sherman'))
unique(data$plot)
data <- rbind(subset(condit, condit$dbh >= 100 & plot != 'cocoli' & plot != 'sherman'))
head(data)
unique(data$plot)
condit_yearly <- aggregate(cbind(dbh,agb,ba)  ~ year + plot, data, sum)
condit_yearly <- rename(condit_yearly, c(sum_dbh = dbh,
sum_agb = agb,
sum_ba = ba))
condit_yearly <- merge(condit_yearly, data)
head(condit_yearly)
condit_yearly <- subset(condit_yearly, select=-c(dbh,agb,ba,hom,stemID,date,gx,gy,sp)) #remove columns that considered individual stem-level changes at sub-year resolution
condit_yearly <- condit_yearly[order(condit_yearly$plot),]
condit_yearly <- unique(condit_yearly)
ggplot(condit_yearly, aes(x=year, y=sum_dbh, color = plot, group=plot)) +
xlab('Date') + ylab('sum DBH') +
ggtitle('sum DBH vs Time')+
geom_line()+
geom_point(size=1)
